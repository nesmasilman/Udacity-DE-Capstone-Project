{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Immigration Data\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "I will use the US Immigration dataset I94 to analyse information about German residents travelling to the USA.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to suppress warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "I'm interested in analysing the behavior of German residents travelling to the USA. I want this project to answer the below questions:\n",
    "\n",
    "* Which types of Visas do Germans use?\n",
    "* what is their travelling patterns over the year?\n",
    "* what are their preferred states to reside in during their visits to USA?\n",
    "\n",
    "To answer these questions, I will use the below datasets:\n",
    "\n",
    "1. US immigration I94 dataset for all months during 2016\n",
    "2. An external JSON file with the countries ISO codes to be able to extract German residents only from the Immigration dataset.\n",
    "3. The US demographic dataset to extract the US states full name, as the states included in immigration datasets are abbreviated.\n",
    "\n",
    "I will be using Pandas and matplotlib libraries for my analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by takeing a look at the sample file to understand the components of Immigration dataset\n",
    "df = pd.read_csv('immigration_data_sample.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2027561</th>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171295</th>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589494</th>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631158</th>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032257</th>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0      1.0   \n",
       "2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0      1.0   \n",
       "589494   1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0      1.0   \n",
       "2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0      1.0   \n",
       "3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0      3.0   \n",
       "\n",
       "        i94addr  depdate   ...     entdepu  matflag  biryear   dtaddto gender  \\\n",
       "2027561      HI  20573.0   ...         NaN        M   1955.0  07202016      F   \n",
       "2171295      TX  20568.0   ...         NaN        M   1990.0  10222016      M   \n",
       "589494       FL  20571.0   ...         NaN        M   1940.0  07052016      M   \n",
       "2631158      CA  20581.0   ...         NaN        M   1991.0  10272016      M   \n",
       "3032257      NY  20553.0   ...         NaN        M   1997.0  07042016      F   \n",
       "\n",
       "        insnum airline        admnum  fltno visatype  \n",
       "2027561    NaN      JL  5.658267e+10  00782       WT  \n",
       "2171295    NaN     *GA  9.436200e+10  XBLNG       B2  \n",
       "589494     NaN      LH  5.578047e+10  00464       WT  \n",
       "2631158    NaN      QR  9.478970e+10  00739       B2  \n",
       "3032257    NaN     NaN  4.232257e+10   LAND       WT  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate',\n",
       "       'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count',\n",
       "       'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu',\n",
       "       'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline',\n",
       "       'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Chatgpt, here are the description of the columns,\n",
    "\n",
    "* CICID: A unique identifier assigned to each individual record in the dataset.\n",
    "* i94yr: The 4-digit year of the visitor's arrival in the United States.\n",
    "* i94mon: The numeric month of the visitor's arrival in the United States.\n",
    "* i94cit: The visitor's country of citizenship.\n",
    "* i94res: The visitor's country of residence.\n",
    "* i94port: The port of entry where the visitor arrived in the United States.\n",
    "* arrdate: The arrival date of the visitor in the United States.\n",
    "* i94mode: The mode of transportation used by the visitor to enter the United States.\n",
    "* i94addr: The state where the visitor intends to reside in the United States.\n",
    "* depdate: The departure date of the visitor from the United States.\n",
    "* i94bir: The visitor's age at the time of arrival in the United States.\n",
    "* i94visa: The type of visa the visitor used to enter theUnited States, such as business, pleasure, or student.\n",
    "* count: The number of people included in the record.\n",
    "* dtadfile: The date on which the data was added to the I94 file.\n",
    "* visapost: The US embassy or consulate where the visitor obtained their visa.\n",
    "* occup: The visitor's occupation.\n",
    "* entdepa: The arrival flag - indicates the reason for the visitor's arrival in the US.\n",
    "* entdepd: The departure flag - indicates the reason for the visitor's departure from the US.\n",
    "* entdepu: Update flag - indicates if the arrival record was updated, typically for a change of address or other similar reasons.\n",
    "* matflag: Match flag - indicates if the arrival and departure records match.\n",
    "* biryear: The visitor's birth year.\n",
    "* dtaddto: The date until which the visitor is admitted to stay in the US.\n",
    "* gender: The visitor's gender.\n",
    "* insnum: \"Immigration and Naturalization Service (INS) number,\" which was a unique identifier assigned by the US government to individuals who applied for immigration benefits or who were admitted to the US as non-immigrant visitors.\n",
    "* airline: The airline used by the visitor to travel to the US.\n",
    "* admnum: Admission number - a unique number assigned to each arrival record.\n",
    "* fltno: The flight number used by the non-immigrant visitor to enter the United States.\n",
    "* visatype: The type of visa that a non-immigrant visitor used to enter the United States."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see that column i94res includes a numerical value which represents the visitor's country of residence. I'm interested in retrieving the names of these countries. So, I uploaded a Json file that contains the ISO numerical code and the corresponding country name. Now let's use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha2</th>\n",
       "      <th>alpha3</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>af</td>\n",
       "      <td>afg</td>\n",
       "      <td>4</td>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  alpha2 alpha3  id         name\n",
       "0     af    afg   4  Afghanistan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes=pd.read_json('countries.json')\n",
    "codes.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm Interested in analysing the data of German residents, so I will get its ISO code from the Codes dataset and use it to filter the data in the immigration dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grmn_cd=codes[codes['name']=='Germany']['id'].iloc[0]\n",
    "grmn_cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the immigration data for each month, and filter only the german residents records. I'm interested in exploring the types of visas column (visatype) and the state in which visitors will reside (i94addr), I will also keep the month column to be used in the analysis. \n",
    "\n",
    "I will ignore the count of persons within the same record that exists in column \"count\", and assume each record represents 1 person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each separate month, I will do 3 steps:\n",
    "\n",
    "1. Filter and keep German residents\n",
    "2. Delete the duplicates of the unique identifier 'cicid'\n",
    "3. Delete the null values, and if their size is large (more than an assumed threshold), a warning message will display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set null values % threshold\n",
    "thrshold= 0.3\n",
    "\n",
    "def monthly_data(month):\n",
    "    \"\"\"Takes in a month dataset and returns a processed dataset with german residents\"\"\"\n",
    "    \n",
    "    data=pd.read_sas('../../data/18-83510-I94-Data-2016/i94_{}16_sub.sas7bdat'.format(month), 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "    data=data[['cicid','i94mon','i94res','i94addr','visatype']]\n",
    "    grmn=data[data['i94res']==grmn_cd].drop_duplicates(subset='cicid')\n",
    "    rto=(len(grmn)-grmn.count().min())/len(grmn)\n",
    "    [print(\"****{} WARNING: Number of Missing Values Exceeds Threshold****\".format(month.upper())) if rto>thrshold else None][0]\n",
    "    grmn.dropna(inplace=True)\n",
    "    return grmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a list with all months\n",
    "months=['jan', 'feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quality check, I will run the monthly_data function on all months and display the first row only just to make sure files were read normally and no month has a high number of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------JAN-----------------\n",
      "       cicid  i94mon  i94res i94addr visatype\n",
      "3136  5028.0     1.0   276.0      NY       F1\n",
      "\n",
      "-----------------FEB-----------------\n",
      "       cicid  i94mon  i94res i94addr visatype\n",
      "1800  2656.0     2.0   276.0      NV       WT\n",
      "\n",
      "-----------------MAR-----------------\n"
     ]
    }
   ],
   "source": [
    "for month in months:\n",
    "    print('-----------------{}-----------------'.format(month.upper()))\n",
    "    print(monthly_data(month).head(1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, We will concatenate all months into one dataset, and drop the \"i94res\" column since it only include German residents' code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "german=pd.concat([monthly_data(month) for month in months], ignore_index=True)\n",
    "german.drop(labels='i94res', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's ensure that the # rows are more than 1 million (project requirement)\n",
    "len(german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's read the U.S. City Demographic Data\n",
    "\n",
    "df_demo=pd.read_csv('us-cities-demographics.csv',delimiter=';')\n",
    "df_demo.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I'm only interested in the columns \"State Code\" & \"State\". I want to use them to form a dict\n",
    "\n",
    "st=df_demo[['State Code','State']].set_index('State Code','State').to_dict()\n",
    "st_dict=st['State']\n",
    "print(len(st_dict))\n",
    "st_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data of 49 US state names and their codes. let's check the codes that exist in the immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_cd=german['i94addr'].value_counts()\n",
    "\n",
    "imc=imm_cd.to_dict()\n",
    "imc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be alot of codes that exist in the immigration dataset while not in the demographic dataset. Let's see the codes of the missing states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no=[(k, imc[k])for k in imc.keys()if k not in st_dict.keys()]\n",
    "no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of records with unknown state name seems to be high, specially for the first 2 states ('GU' & 'MP'). We will not be able to drop them since they represent around 25% of the data. Since we are building a pipeline, then it's important to avoid hard coding steps by avoiding manual search for these two values. The solution will be to use a new dataset that contains the state name and the corresponding state code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_code=pd.read_csv('georef-united-states-of-america-zc-point@public.csv',delimiter=';')\n",
    "full_code.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the required dictionary from our new dataset\n",
    "\n",
    "stat=full_code[['Official USPS State Code','Official State Name']].set_index('Official USPS State Code','Official State Name').to_dict()\n",
    "stat_dict=stat['Official State Name']\n",
    "len(stat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again, let's see if there is a missing state code\n",
    "non=[(k, imc[k])for k in imc.keys()if k not in stat_dict.keys()]\n",
    "non"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the number of states with unknown code is smaller. I will replace these states' names with 'Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict.update({key: 'Others' for key in imc.keys() if key not in stat_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stat_dict))\n",
    "stat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the full state name and add it to the german dataset\n",
    "german['State']= german['i94addr'].apply(lambda x: stat_dict[x])\n",
    "german.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to display a warning if the number of 'Others' state exceeded a certain threshold, \n",
    "# this will be helpful if the file was updated to ensure that the # of others will not be large in the new records.\n",
    "\n",
    "st_thrsh=0.15\n",
    "others_ratio=german[german['State']=='Others']['State'].count()/len(german)\n",
    "\n",
    "[print(\"***** WARNING: Number of unknown (Others) states Exceeds Threshold****\") if others_ratio>st_thrsh else None][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Now, our dataset has reached its final form and is now ready for our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's answer our questions:\n",
    "Which types of Visas do Germans use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visas=german['visatype'].value_counts()\n",
    "visas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(visas.index), list(visas))\n",
    "plt.title('Types of Visas used by German residents during their visit to USA')\n",
    "plt.xlabel('Visa Type')\n",
    "plt.ylabel('Visa Count')\n",
    "plt.xticks(rotation=90)\n",
    "#ax.bar_label(bars)\n",
    "plt.box(False)\n",
    "#plt.gca().invert_yaxis();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that \"WT\" & \"GMT\" are the most populat visa types. Now let's check their travelling pattern over the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat=german['i94mon'].value_counts()\n",
    "pat.sort_index(inplace=True)\n",
    "pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(x=pat.index, height=pat)\n",
    "plt.title(\"Number of German Residents' Visits to USA per Month\")\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('count');\n",
    "plt.xticks(pat.index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count over month is a bit similar with no spikes (except for a decrease in March & April)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what are their preferred states to reside in during their visits to USA? I will display the top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref=german['State'].value_counts()\n",
    "pref[:10].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.bar(list(pref[:10].index), list(pref[:10]))\n",
    "\n",
    "plt.title('Top 10 Preferred States' +  \"\\n\" + 'by German Residents', loc='right')\n",
    "plt.xlabel('States')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.box(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that Guam & California are the go-to states for German residents.\n",
    "\n",
    "Now we have answers for our questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The data model used in this project is Star Schema. The reason for choosing it is that the datasets used are one fact table (Immigration dataset), and 2 dimensions tables (country ISO codes & states abbrev. table) which were used to complete the data in the main Immigration dataset and make it reach to a form that will be ready for analysis.\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "1. Retrieve the immigration data from its source\n",
    "2. Create a function (monthly_data) that takes in raw data files, does the required pre-processing, and displays the needed warnings-if any-.\n",
    "3. Use dimention table (ISO code) to filter the required data (German residents) \n",
    "4. Join the fact table with dimentions table to retrieve the required data (State full name), and display a warning if the number of unknown states increase, so that in future runs after the files update, we could ensure that we don't have a huge number of missing state name. We also used \"Others\" to fill missing/incorrect state name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "\n",
    "Build the data pipelines to create the data model: Already done in the above steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks:\n",
    "\n",
    "Check #1: Checking the % of null values before dropping them, and displaying an error message if the % exceeded a given threshold.\n",
    "\n",
    "Check #2: Checking the % of states with unknow names (refereed to as \"others\") and displaying a warning message if the % exceeded a given threshold.\n",
    "\n",
    "Check #3: Dropping duplicated rows using the unique identifier column \"cicid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file (Included in a separate file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "I used only Jupyter notebook for the project, since the data size was still small and all project stages could be successfully run through the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "From the analytics point of view, this data should be run once per month, since the analysis was built on month granularity. So any change in data behavior should be detected on the month level not on weekly or daily...etc specially that the dataset doesn't include columns for weeks or days timestamp. The smallest time level included is the month column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " \n",
    " I would upload the datasets to AWS and use it for analysis, since the data size will increase and it won't be efficient to run it in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    "I would use airflow and create a DAG with a scheduled daily run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * The database needed to be accessed by 100+ people.\n",
    " \n",
    " I would upload all datasets on the cloud and use its storage capacity, and create different roles with different access for each group to be able to access the data with their proposed limitation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
